{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/MorvanZhou/pytorch-A3C/blob/master/discrete_A3C.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reinforcement Learning (A3C) using Pytroch + multiprocessing.\n",
    "The most simple implementation for continuous action.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from MorvanUtils import v_wrap, set_init, push_and_pull, record\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "# from shared_adam import SharedAdam\n",
    "import gym\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "UPDATE_GLOBAL_ITER = 10\n",
    "GAMMA = 0.9\n",
    "MAX_EP = 4000\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "N_S = env.observation_space.shape[0]\n",
    "N_A = env.action_space.n\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Shared optimizer, the parameters in the optimizer will shared in the multiprocessors.\n",
    "\"\"\"\n",
    "class SharedAdam(torch.optim.Adam):\n",
    "  \n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.9), eps=1e-8,\n",
    "                 weight_decay=0):\n",
    "        super(SharedAdam, self).__init__(params, lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        # State initialization\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                state['step'] = 0\n",
    "                state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "\n",
    "                # share in memory\n",
    "                state['exp_avg'].share_memory_()\n",
    "                state['exp_avg_sq'].share_memory_()\n",
    "\n",
    "\"DQN training on cart pole\"\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.s_dim = s_dim\n",
    "        self.a_dim = a_dim\n",
    "        self.pi1 = nn.Linear(s_dim, 200)\n",
    "        self.pi2 = nn.Linear(200, a_dim)\n",
    "        self.v1 = nn.Linear(s_dim, 100)\n",
    "        self.v2 = nn.Linear(100, 1)\n",
    "        set_init([self.pi1, self.pi2, self.v1, self.v2])\n",
    "        self.distribution = torch.distributions.Categorical\n",
    "\n",
    "    def forward(self, x):\n",
    "        pi1 = F.relu6(self.pi1(x))\n",
    "        logits = self.pi2(pi1)\n",
    "        v1 = F.relu6(self.v1(x))\n",
    "        values = self.v2(v1)\n",
    "        return logits, values\n",
    "\n",
    "    def choose_action(self, s):\n",
    "        self.eval()\n",
    "        logits, _ = self.forward(s)\n",
    "        prob = F.softmax(logits, dim=1).data\n",
    "        m = self.distribution(prob)\n",
    "        return m.sample().numpy()[0]\n",
    "\n",
    "    def loss_func(self, s, a, v_t):\n",
    "        self.train()\n",
    "        logits, values = self.forward(s)\n",
    "        td = v_t - values\n",
    "        c_loss = td.pow(2)\n",
    "        \n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        m = self.distribution(probs)\n",
    "        exp_v = m.log_prob(a) * td.detach().squeeze()\n",
    "        a_loss = -exp_v\n",
    "        total_loss = (c_loss + a_loss).mean()\n",
    "        return total_loss\n",
    "\n",
    "\n",
    "class Worker(mp.Process):\n",
    "    def __init__(self, gnet, opt, global_ep, global_ep_r, res_queue, name):\n",
    "        super(Worker, self).__init__()\n",
    "        self.name = 'w%i' % name\n",
    "        self.g_ep, self.g_ep_r, self.res_queue = global_ep, global_ep_r, res_queue\n",
    "        self.gnet, self.opt = gnet, opt\n",
    "        self.lnet = Net(N_S, N_A)           # local network\n",
    "        self.env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "    def run(self):\n",
    "        total_step = 1\n",
    "        while self.g_ep.value < MAX_EP:\n",
    "            s = self.env.reset()\n",
    "            buffer_s, buffer_a, buffer_r = [], [], []\n",
    "            ep_r = 0.\n",
    "            while True:\n",
    "                if self.name == 'w0':\n",
    "                    self.env.render()\n",
    "                a = self.lnet.choose_action(v_wrap(s[None, :]))\n",
    "                s_, r, done, _ = self.env.step(a)\n",
    "                if done: r = -1\n",
    "                ep_r += r\n",
    "                buffer_a.append(a)\n",
    "                buffer_s.append(s)\n",
    "                buffer_r.append(r)\n",
    "\n",
    "                if total_step % UPDATE_GLOBAL_ITER == 0 or done:  # update global and assign to local net\n",
    "                    # sync\n",
    "                    push_and_pull(self.opt, self.lnet, self.gnet, done, s_, buffer_s, buffer_a, buffer_r, GAMMA)\n",
    "                    buffer_s, buffer_a, buffer_r = [], [], []\n",
    "\n",
    "                    if done:  # done and print information\n",
    "#                         record(self.g_ep, self.g_ep_r, ep_r, self.res_queue, self.name)\n",
    "                        break\n",
    "                s = s_\n",
    "                total_step += 1\n",
    "        self.res_queue.put(None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnet = Net(N_S, N_A)        # global network\n",
    "gnet.share_memory()         # share the global parameters in multiprocessing\n",
    "opt = SharedAdam(gnet.parameters(), lr=0.0001)      # global optimizer\n",
    "global_ep, global_ep_r, res_queue = mp.Value('i', 0), mp.Value('d', 0.), mp.Queue()\n",
    "\n",
    "# parallel training\n",
    "workers = [Worker(gnet, opt, global_ep, global_ep_r, res_queue, i) for i in range(mp.cpu_count())]\n",
    "[w.start() for w in workers]\n",
    "res = []                    # record episode reward to plot\n",
    "while True:\n",
    "    r = res_queue.get()\n",
    "    if r is not None:\n",
    "        res.append(r)\n",
    "    else:\n",
    "        break\n",
    "[w.join() for w in workers]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(res)\n",
    "plt.ylabel('Moving average ep reward')\n",
    "plt.xlabel('Step')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
